#include "sort.h"

#include "prefix_sum.h"

// inline keyword avoids multiple definition errors, but produces warnings.
#define TEMPLATE_COMPILATION __inline__

template<typename O, typename A>
__device__ __host__ void bubble_sort(uint32_t *keys, O *values, A *second_values, size_t size)
{
  // Bubble sort the way out of the array.
  for (size_t i = size - 1; i > 0; --i)
  {
    for (size_t j = 0; j < i; ++j)
    {
      if (keys[j + 1] < keys[j])
      {
        O tmp_value = values[j];
        A tmp_second_value = second_values[j];
        uint32_t tmp_key = keys[j];

        values[j] = values[j + 1];
        second_values[j] = second_values[j + 1];
        keys[j] = keys[j + 1];

        values[j + 1] = tmp_value;
        second_values[j + 1] = tmp_second_value;
        keys[j + 1] = tmp_key;
      }
    }
  }
}


template<typename O, typename A>
void single_thread_bubble_sort(uint32_t *keys, O *values, A *second_values, size_t size)
{
  uint32_t *cpu_keys = new uint32_t[size];
  O *cpu_values = new O[size];
  A *cpu_second_values = new A[size];

  cudaMemcpy(cpu_keys, keys, sizeof(uint32_t) * size, cudaMemcpyDefault);
  cudaMemcpy(cpu_values, values, sizeof(O) * size, cudaMemcpyDefault);
  cudaMemcpy(cpu_second_values, second_values, sizeof(A) * size, cudaMemcpyDefault);

  bubble_sort(cpu_keys, cpu_values, cpu_second_values, size);

  cudaMemcpy(keys, cpu_keys, sizeof(uint32_t) * size, cudaMemcpyDefault);
  cudaMemcpy(values, cpu_values, sizeof(O) * size, cudaMemcpyDefault);
  cudaMemcpy(second_values, cpu_second_values, sizeof(A) * size, cudaMemcpyDefault);

  delete[] cpu_keys;
  delete[] cpu_values;
  delete[] cpu_second_values;
}

// How many bits are done per iteration
#define RADIX_SORT_BITS_PER_BLOCK 2
// How many values are present in the defined block.
#define RADIX_SORT_BITS_VALUES (1 << RADIX_SORT_BITS_PER_BLOCK)
// What must be the size of the values block that are done per cuda block.
// This also correspond to the number of threads that will be
// launched per blocks, so this number must not be above 1024.
#define RADIX_SORT_BLOCK_SIZE 128

TEMPLATE_COMPILATION __device__ size_t get_key_part(size_t key, size_t offset)
{
  return (key >> offset) & (RADIX_SORT_BITS_VALUES - 1);
}

// Create the mask of the part.
__inline__  __global__ void radix_sort_frequency(
  const uint32_t *const keys,
  size_t size,
  size_t offset,
  size_t *frequency)
{

  size_t index = blockIdx.x * blockDim.x + threadIdx.x;
  if (index >= size) return; // Nothing to do here

  frequency[size * get_key_part(keys[index], offset) + index] = 1;

}

TEMPLATE_COMPILATION __global__ void radix_sort_global_frequency(
  const size_t *const frequency,
  size_t size,
  size_t *global_frequency)
{
  static_assert(RADIX_SORT_BITS_VALUES <= 1024, "All the computation must happen in one block");

  size_t index = threadIdx.x;
  if (index >= RADIX_SORT_BITS_VALUES) return;// Nothing to do here

  global_frequency[index] = frequency[(index + 1) * size - 1];
}

template<typename O, typename A>
TEMPLATE_COMPILATION __global__ void radix_sort_move_all(
  const uint32_t *const input_keys,
  uint32_t *output_keys,
  const O *const input_values,
  O *output_values,
  const A *const input_second_values,
  A *output_second_values,
  size_t size,
  size_t offset,
  const size_t *const frequency,
  const size_t *const global_frequency)
{
  size_t index = blockIdx.x * blockDim.x + threadIdx.x;
  if (index >= size) return; // Nothing to do here

  // Save all key and value as the array will be completly rewritten
  uint32_t current_key = input_keys[index];
  O current_value = input_values[index];
  A current_second_value = input_second_values[index];


  // Find where to store the new key and value
  size_t key_part = get_key_part(current_key, offset);
  size_t new_index =
    // How much place takes the last one
    (key_part == 0 ? 0 : global_frequency[key_part - 1])
    // Where to place the current value, we need to substract one
    // as the frequency tell the number (1-based indexing),
    // whereas we need the index (0-based indexing)
    + frequency[size * key_part + index] - 1;

  // Store the keys and the values to their new position
  output_keys[new_index] = current_key;
  output_values[new_index] = current_value;
  output_second_values[new_index] = current_second_value;
}

template<typename O, typename A>
void parallel_radix_sort(uint32_t *keys, O *values, A *second_values, size_t size)
{
  size_t nb_blocks = ceil((float)size / (float)RADIX_SORT_BLOCK_SIZE);


  // Synchronisation can't happen between multiple blocks together,
  // so keys and values are double buffered:
  // one is read and the other is written to.
  uint8_t current_buffer = 0;
  uint32_t *double_keys[2] = { keys, nullptr };
  O *double_values[2] = { values, nullptr };
  A *double_second_values[2] = { second_values, nullptr };

  cudaMalloc(&double_keys[1], sizeof(uint32_t) * size);
  cudaMalloc(&double_values[1], sizeof(O) * size);
  cudaMalloc(&double_second_values[1], sizeof(O) * size);


  // Allocate a buffer of all prefix sum
  size_t frequency_size = size * RADIX_SORT_BITS_VALUES;
  size_t *frequency;
  cudaMalloc(&frequency, sizeof(size_t) * frequency_size);

  size_t *global_frequency;
  cudaMalloc(&global_frequency, sizeof(size_t) * RADIX_SORT_BITS_VALUES);

  // Perform the radix sort, block by block.
  // The sort is least significant digit so that it can be stable.
  for (size_t offset = 0;
       offset < sizeof(uint32_t) * 8;
       offset += RADIX_SORT_BITS_PER_BLOCK, current_buffer = (current_buffer + 1) % 2)
  {

    // Clear the frequency and global_frequency as they might contains
    // the previous iteration results, and all functions expects a zeroed array.
    cudaMemset(frequency, 0, sizeof(size_t) * frequency_size);
    cudaMemset(global_frequency, 0, sizeof(size_t) * RADIX_SORT_BITS_VALUES);

    // No order check is performed, as the key distribution means that
    // they will not be ordered until the last iteration.


    // Compute the histogram per block, and save it in the prefix sum
    radix_sort_frequency<<<nb_blocks, RADIX_SORT_BLOCK_SIZE>>>(
      double_keys[current_buffer], size, offset, frequency
    );

    // Compute the prefix sum of each histograms,
    // So that it is known here the position want to be sorted.
    for (size_t i = 0; i < RADIX_SORT_BITS_VALUES; ++i)
    {// Compute the prefix sum of all frequencies independently
      shared_prefix_sum(frequency + (size * i), size);

      // TODO: use stream ???
    }

    // Compute the general prefix_sum, so that each object know where they need to be placed.
    radix_sort_global_frequency<<<1, RADIX_SORT_BITS_VALUES>>>(frequency, size, global_frequency);
    shared_prefix_sum(global_frequency, RADIX_SORT_BITS_VALUES);

    // Once all computation have been done, move the keys and values to their expected place.
    radix_sort_move_all<<<nb_blocks, RADIX_SORT_BLOCK_SIZE>>>(
      double_keys[current_buffer], double_keys[(current_buffer + 1) % 2],
      double_values[current_buffer], double_values[(current_buffer + 1) % 2],
      double_second_values[current_buffer], double_second_values[(current_buffer + 1) % 2],
      size, offset, frequency, global_frequency
    );
  }

  if (current_buffer == 1)
  {
    // Move back the keys and values to the current buffer
    cudaMemcpy(keys, double_keys[1], sizeof(uint32_t) * size, cudaMemcpyDeviceToDevice);
    cudaMemcpy(values, double_values[1], sizeof(O) * size, cudaMemcpyDeviceToDevice);
    cudaMemcpy(second_values, double_second_values[1], sizeof(A) * size, cudaMemcpyDeviceToDevice);
  }

  cudaFree(frequency);
  cudaFree(global_frequency);
  cudaFree(double_keys[1]);
  cudaFree(double_values[1]);
  cudaFree(double_second_values[1]);
}